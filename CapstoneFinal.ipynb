{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clean, new dataframe\n",
    "Dataframe = pd.DataFrame()\n",
    "#open and create dataframe\n",
    "Dataframe =pd.read_excel('CapstoneData.xlsx', sheet_name='Campaign Results')\n",
    "\n",
    "#set index as customer number\n",
    "Dataframe.set_index('Customer Number', inplace=True)\n",
    "\n",
    "#drop negative period sales as irrelevent\n",
    "Dataframe.drop(Dataframe[Dataframe['Campaign Period Sales']<0].index, inplace=True)\n",
    "\n",
    "#Drop negative Historical Sales Volume\n",
    "Dataframe.drop(Dataframe[Dataframe['Historical Sales Volume']<0].index, inplace=True)\n",
    "\n",
    "#drop row with probably incorrect date\n",
    "Dataframe['Date of First Purchase'].min()\n",
    "Dataframe = Dataframe.drop(Dataframe[Dataframe['Date of First Purchase']=='1926-01-15 00:00:00'].index)\n",
    "#print(Dataframe['Date of First Purchase'].min())\n",
    "\n",
    "#Drop outlier number of prior year sales\n",
    "Dataframe = Dataframe.drop(Dataframe[Dataframe['Number of Prior Year Transactions']==313].index)\n",
    "#convert Y YY N to True False\n",
    "names = ('Desk', 'Executive Chair', 'Standard Chair', 'Monitor', 'Printer', 'Computer', 'Insurance', 'Toner', 'Office Supplies')\n",
    "\n",
    "for name in names:\n",
    "    #print(name)\n",
    "    #print(Dataframe[name].unique())\n",
    "\n",
    "    Dataframe[name].fillna(\"N\", inplace =True)\n",
    "    #print(Dataframe[name].unique())\n",
    "    Dataframe[name] = np.where(Dataframe[name].str.contains(\"Y\"), True, False)\n",
    "\n",
    "#convert categorical number of employees to integer\n",
    "#print(Dataframe['Number of Employees'].unique())\n",
    "#dictx = {'1-5':3,'6-10':8, '11-50':30, '51-100':75, '101-500':300, '500+':750, ' ':np.nan}\n",
    "dictx = {'1-5':1,'6-10':2, '11-50':3, '51-100':4, '101-500':5, '500+':6, ' ':np.nan}\n",
    "Dataframe['Number Employees'] = Dataframe['Number of Employees'].map(dictx) \n",
    "Dataframe = Dataframe.drop(columns='Number of Employees')\n",
    "#fill in missing values with 8, because most common value (25 and 50 percentile)\n",
    "Dataframe[\"Number Employees\"].fillna(8, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert date of 1st purchase to quartile\n",
    "from datetime import date\n",
    "#print(Dataframe['Date of First Purchase'].dt.year.min())\n",
    "Dataframe['ClientYear1stPurch'] = Dataframe['Date of First Purchase'].dt.year.max() - Dataframe['Date of First Purchase'].dt.year\n",
    "\n",
    "Dataframe['Client Tenure'] =Dataframe['ClientYear1stPurch']*4//(Dataframe['ClientYear1stPurch'].max()+1)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataframe = Dataframe.drop(columns = ['Date of First Purchase', 'ClientYear1stPurch'])\n",
    "\n",
    "#drop irrelevent columns\n",
    "Dataframe = Dataframe.drop(columns = 'Repurchase Method')\n",
    "Dataframe = Dataframe.drop(columns = 'Last Transaction Channel')\n",
    "#lanuage is 98% English or unknown.  Unknown is not reliable information.  thus we drop it.\n",
    "Dataframe = Dataframe.drop(columns = 'Language')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create first y as Booleon sales\n",
    "Dataframe['Sales_bool'] = np.where(Dataframe['Campaign Period Sales']>0, True, False)\n",
    "\n",
    "\n",
    "Buyers = Dataframe.loc[Dataframe['Campaign Period Sales']>0]\n",
    "Non_buyers = Dataframe.loc[Dataframe['Campaign Period Sales']==0]\n",
    "Dataframe['Sales_num'] = Dataframe['Campaign Period Sales']\n",
    "Dataframe = Dataframe.drop(columns = ['Campaign Period Sales'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#now that they are extracted, drop columns\n",
    "#Dataframe = Dataframe.drop(columns = ['Campaign Period Sales', 'Sales'])\n",
    "#Buyers = Buyers.drop(columns = ['Campaign Period Sales','Sales'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe = Dataframe.drop(columns = 'Office Supplies')\n",
    "\"\"\"\n",
    "#drop other potentially useless columns - did not use this in final version\n",
    "\n",
    "\n",
    "Dataframe = Dataframe.drop(columns=\n",
    "['Do Not Direct Mail Solicit', 'Do Not Email', 'Do Not Telemarket',\n",
    "       'Desk', 'Executive Chair',  'Insurance', 'Toner', 'Office Supplies'])\n",
    "       \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe = Dataframe.drop(columns=['Computer', 'Printer'])\n",
    "Dataframe.info()\n",
    "Dataframe.describe()\n",
    "print(Dataframe.iloc[0, 0:-2])\n",
    "print(Dataframe.iloc[0, -2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Reserve set for testing\n",
    "Dataframe_train, Dataframe_reserve, y_df_train, y_reserve = train_test_split(Dataframe.iloc[:, 0:-2], Dataframe.iloc[:, -2:-1] , test_size=.25)\n",
    "\n",
    "\n",
    "#train test split for Predicting Buyers\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train,  y_test = train_test_split(Dataframe_train, y_df_train , test_size=.5)\n",
    "#Make y_train_amt y_test_amt for LinearRegression\n",
    "y_train_amt = y_train\n",
    "y_train_amt = y_train_amt.merge(pd.DataFrame(Dataframe['Sales_num']), left_index=True, right_index=True)\n",
    "y_train_amt.drop(columns = 'Sales_bool', inplace=True)\n",
    "\n",
    "y_test_amt = y_test\n",
    "y_test_amt = y_test_amt.merge(pd.DataFrame(Dataframe['Sales_num']), left_index=True, right_index=True)\n",
    "y_test_amt.drop(columns = 'Sales_bool', inplace=True)\n",
    "\n",
    "#scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train_scale, y_train)\n",
    "\n",
    "y_train_sub = np.ravel(y_train_sub)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=50, learning_rate = .1, max_features=6, max_depth = 5)\n",
    "gb.fit(X_train_sub, y_train_sub)\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
    "print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
    "\n",
    "\n",
    "predictions = gb.predict(X_test_scale)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "feature_importances =  gb.feature_importances_\n",
    "features = X_train.columns\n",
    "feats= pd.DataFrame({'features':features, 'importance':feature_importances})\n",
    "plt.figure(figsize =(8,2))\n",
    "plt.bar(feats.features, feats.importance, color='lightblue')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "labels = ['True Positives', 'False Positives']\n",
    "sizes = [cm[1,1], cm[0,1]]\n",
    "colors = ['gold', 'lightskyblue']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=90)\n",
    "plt.pie(sizes, colors=colors, shadow=True, startangle=90,autopct='%1.1f%%')\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "labels = ['True Negatives', 'False Negatives']\n",
    "sizes = [cm[0,0], cm[1,0]]\n",
    "colors = ['gold','lightskyblue']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=90)\n",
    "plt.pie(sizes, colors=colors, shadow=True, startangle=90,autopct='%1.1f%%')\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "Chance_of_Sale = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "print(Chance_of_Sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real_Buyers_train = pd.DataFrame(X_train.iloc[:, 0:17])\n",
    "Real_Buyers_train['IsBuyer'] = y_train\n",
    "Real_Buyers_train.drop(Real_Buyers_train[Real_Buyers_train['IsBuyer']==False].index, inplace=True)\n",
    "Real_Buyers_train = Real_Buyers_train.merge(pd.DataFrame(Dataframe['Sales_num']), left_index=True, right_index=True)\n",
    "y_train_real = Real_Buyers_train['Sales_num']\n",
    "Real_Buyers_train.drop(columns = 'IsBuyer', inplace=True)\n",
    "Real_Buyers_train.drop(columns = 'Sales_num',inplace=True)\n",
    "\n",
    "\n",
    "Expected_Buyers_test = pd.DataFrame(X_test.iloc[:, 0:17])\n",
    "Expected_Buyers_test['Pred']=predictions\n",
    "#Expected_Buyers_test.drop(Expected_Buyers_test[Expected_Buyers_test['Pred']==False].index,inplace=True)\n",
    "Expected_Buyers_test = Expected_Buyers_test.merge(pd.DataFrame(Dataframe['Sales_num']), left_index=True, right_index=True)\n",
    "y_test_exp= Expected_Buyers_test['Sales_num']\n",
    "Expected_Buyers_test.drop(columns = 'Pred', inplace=True)\n",
    "Expected_Buyers_test.drop(columns = 'Sales_num',inplace=True)\n",
    "#Expected_Buyers_test.drop(columns = 'Sales_Amt',inplace=True)\n",
    "\n",
    "Real_Buyers_train.info()\n",
    "Expected_Buyers_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#train test split for LinReg (buyers only)\n",
    "Buyers = Dataframe.loc[Dataframe['Sales_num']>0]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_buy, X_test_buy, y_train_buy,  y_test_buy = train_test_split(Buyers.iloc[:, 0:-2], Buyers.iloc[:, -1], test_size=.5)\n",
    "#train_test split for all\n",
    "X_train_all, X_test_all, y_train_all,  y_test_all = train_test_split(Dataframe.iloc[:, 0:-2], Dataframe.iloc[:, -1], test_size=.5)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, r2_score, median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler= MinMaxScaler()\n",
    "X_train_scale_buy = scaler.fit_transform(Real_Buyers_train)\n",
    "#X_test_scale_buy = scaler.transform(Expected_Buyers_test)\n",
    "X_test_scale_buy = scaler.transform(Expected_Buyers_test)\n",
    "model_buy=LinearRegression().fit(X_train_scale_buy, y_train_real)\n",
    "\n",
    "y_pred_buy = model_buy.predict(X_test_scale_buy)\n",
    "#y_pred_non_buyers = model_buy.predict(X_test_scale_non_buy_pred)\n",
    "print(r2_score(y_test_exp, y_pred_buy))\n",
    "\n",
    "print(median_absolute_error(y_test_exp, y_pred_buy))\n",
    "\n",
    "\n",
    "print(list(zip(X_train.columns, model_buy.coef_)))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "feature_importances =  model_buy.coef_\n",
    "features = Real_Buyers_train.columns\n",
    "feats= pd.DataFrame({'features':features, 'importance':feature_importances})\n",
    "plt.figure(figsize =(8,2))\n",
    "plt.bar(feats.features, feats.importance, color='lightblue')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "start = 0\n",
    "end = 1000\n",
    "\n",
    "plt.scatter(y_pred_buy[start:end, ], y_test_buy[start:end, ],  color='gray')\n",
    "#plt.plot(X_test, y_pred, color='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(y_pred_all[start:end, ], y_test_all[start:end, ],  color='gray')\n",
    "#plt.plot(X_test, y_pred, color='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Lift= pd.DataFrame()\n",
    "Lift = pd.DataFrame(X_test)\n",
    "Lift['Pred']=predictions\n",
    "Lift['Sales_Amt']= Dataframe['Sales_num'].loc[Lift.index & Dataframe.index]\n",
    "Temp = pd.DataFrame(Expected_Buyers_test)\n",
    "Temp['Pred_Amt'] = y_pred_buy\n",
    "\n",
    "Lift['Pred_Amt'] = Temp['Pred_Amt'].loc[Lift.index & Temp.index]\n",
    "\n",
    "Lift['Pred_Amt'].fillna(0, inplace=True)\n",
    "\n",
    "Lift['Exp_Profit'] = Lift['Pred_Amt']*.22*Chance_of_Sale - np.where(Lift['Pred_Amt']>0, 8.4, 0)*Chance_of_Sale - 45.65\n",
    "#Lift['Actual_Profit'] = Lift['Sales_Amt']*.22 - np.where(Lift['Sales_Amt']>0, 8.4, 0) - 45.65\n",
    "\n",
    "Lift['Sales_bool'] = np.where(Lift['Sales_Amt']>0, True, False)\n",
    "Lift['GrossProfit'] = Lift['Sales_Amt']*.22\n",
    "Lift['Trans_cost'] = Lift['Sales_bool']*8.4\n",
    "Lift['Camp_cost'] = 45.65\n",
    "Lift['Actual_Profit']= Lift['GrossProfit']-Lift['Trans_cost']-Lift['Camp_cost']\n",
    "\n",
    "\n",
    "\n",
    "Lift_Pred_Sale= Lift[(Lift['Pred']==True) & (Lift['Pred_Amt']>1)]\n",
    "Lift_Pred_Sale['decile'] = pd.qcut(Lift_Pred_Sale['Pred_Amt']*-1, 10, labels=False,duplicates='drop')+1\n",
    "Lift_Pred_No_Sale =Lift.loc[((Lift['Pred']==True) & (Lift['Pred_Amt']<1)) |  (Lift['Pred']==False)]  \n",
    "\n",
    "#Lift_Pred_No_Sale.drop(Lift.index == isin(Lift_Pred_Sale.index), inplace = True)\n",
    "Lift_Pred_No_Sale['decile'] = pd.qcut(Lift_Pred_No_Sale['Pred_Amt']*-1, 10, labels=False,duplicates='drop')+1\n",
    "\n",
    "#@song0089 - Ya, so use df = df[df.index.isin(df1.index)]\n",
    "#Lift_Pred_Sale.head(20)\n",
    "#Lift.info()  Sales_Amt\tPred_Amt\tExp_Profit\tActual_Profit\n",
    "#Lift_Pred_Sale.groupby(\"decile\").min()\n",
    "#Lift_Pred_No_Sale.groupby('decile').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Pred_Sale_means = Lift_Pred_Sale.groupby(\"decile\").mean()\n",
    "Pred_Sale_sum = Lift_Pred_Sale.groupby(\"decile\").sum()\n",
    "Pred_No_Sale_sum = Lift_Pred_No_Sale.groupby(\"decile\").sum()\n",
    "Pred_Sale_sum\n",
    "#Pred_Sale_means.loc[Pred_Sale_means['Exp_Profit']>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lift_Pred_No_Sale.groupby(\"decile\").min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clmnames= Lift.columns\n",
    "for name in clmnames:\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_axes([0,0,1,1])\n",
    "    ax.scatter(Lift[name], Lift['Pred_Amt'], color='r')\n",
    "    #ax.scatter(grades_range, boys_grades, color='b')\n",
    "    ax.set_xlabel(name)\n",
    "    ax.set_ylabel('Sales')\n",
    "    ax.set_title('scatter plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_Sale_sum.to_excel('Pred_Sale_sum.xlsx')\n",
    "\n",
    "Pred_No_Sale_sum.to_excel('Pred_No_Sale_sum.xlsx')\n",
    "Lift.to_excel('Lift.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lift_Pred_Sale[Lift_Pred_Sale['decile']<7]\n",
    "Lift_Pred_Sale['Targeted']=np.where(Lift_Pred_Sale['decile']<7, True, False)\n",
    "Lift_Pred_No_Sale['Targeted']=np.where(Lift_Pred_No_Sale['decile']<3, True, False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lift_Pred_Sale['Targeted']=np.where(Lift_Pred_Sale['decile']<7, True, False)\n",
    "#Lift_Pred_No_Sale['Targeted']=np.where(Lift_Pred_No_Sale['decile']<3, True, False )\n",
    "\n",
    "#frames = [Lift_Pred_No_Sale, Lift_Pred_Sale]\n",
    "#All= pd.concat(frames)\n",
    "All_sum = All.mean().transpose()\n",
    "All_sum_Targ = All.groupby(['Targeted']).mean()\n",
    "All_sum_Targ.to_excel('All_sum_Targ.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now check on resrverd data\n",
    "# Dataframe_reserve, y_reserve \n",
    "Dataframe_reserve_scale = scaler.transform(Dataframe_reserve.iloc[:, 0:16])\n",
    "\n",
    "predictions_reserved = gb.predict(Dataframe_reserve_scale)\n",
    "y_pred_res = model_buy.predict(Dataframe_reserve_scale)\n",
    "Lift= pd.DataFrame()\n",
    "Lift = pd.DataFrame(Dataframe_reserve)\n",
    "Lift['Pred']=predictions_reserved\n",
    "Lift['Sales_Amt']= Dataframe['Sales_num'].loc[Lift.index & Dataframe.index]\n",
    "\n",
    "Lift['Pred_Amt'] = y_pred_res\n",
    "Lift['Pred_Amt'].fillna(0, inplace=True)\n",
    "\n",
    "Lift['Exp_Profit'] = Lift['Pred_Amt']*.22*Chance_of_Sale - np.where(Lift['Pred_Amt']>0, 8.4, 0)*Chance_of_Sale - 45.65\n",
    "#Lift['Actual_Profit'] = Lift['Sales_Amt']*.22 - np.where(Lift['Sales_Amt']>0, 8.4, 0) - 45.65\n",
    "\n",
    "Lift['Sales_bool'] = np.where(Lift['Sales_Amt']>0, True, False)\n",
    "Lift['GrossProfit'] = Lift['Sales_Amt']*.22\n",
    "Lift['Trans_cost'] = Lift['Sales_bool']*8.4\n",
    "Lift['Camp_cost'] = 45.65\n",
    "Lift['Actual_Profit']= Lift['GrossProfit']-Lift['Trans_cost']-Lift['Camp_cost']\n",
    "\n",
    "\n",
    "\n",
    "Lift_Pred_Sale= Lift[(Lift['Pred']==True) & (Lift['Pred_Amt']>1)]\n",
    "Lift_Pred_Sale['decile'] = pd.qcut(Lift_Pred_Sale['Pred_Amt']*-1, 10, labels=False,duplicates='drop')\n",
    "Lift_Pred_No_Sale =Lift.loc[((Lift['Pred']==True) & (Lift['Pred_Amt']<1)) |  (Lift['Pred']==False)]  \n",
    "\n",
    "#Lift_Pred_No_Sale.drop(Lift.index == isin(Lift_Pred_Sale.index), inplace = True)\n",
    "Lift_Pred_No_Sale['decile'] = pd.qcut(Lift_Pred_No_Sale['Pred_Amt']*-1, 10, labels=False,duplicates='drop')+1\n",
    "\n",
    "Pred_Sale_means = Lift_Pred_Sale.groupby(\"decile\").mean()\n",
    "Pred_Sale_sum = Lift_Pred_Sale.groupby(\"decile\").sum()\n",
    "Pred_No_Sale_sum = Lift_Pred_No_Sale.groupby(\"decile\").sum()\n",
    "Pred_No_Sale_sum\n",
    "#Pred_Sale_means.loc[Pred_Sale_means['Exp_Profit']>0]\n",
    "#Lift_Pred_No_Sale.groupby(\"decile\").count()\n",
    "#Lift.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#used to determine best parameters for GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# train with Gradient Boosting algorithm\n",
    "#train test split for Predicting Buyers\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train,  y_test = train_test_split(Dataframe.iloc[:, 0:-2], Dataframe.iloc[:, -2:-1] , test_size=.5)\n",
    "\n",
    "# compute the accuracy scores on train and validation sets when training with different learning rates\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "# split training feature and target sets into training and validation subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train_scale, y_train, random_state=0)\n",
    "\n",
    "y_train_sub = np.ravel(y_train_sub)\n",
    "y_validation_sub=np.ravel(y_validation_sub)\n",
    "learning_rates = [.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for learning_rate in learning_rates:\n",
    "    gb = GradientBoostingClassifier(n_estimators=50, learning_rate = learning_rate, max_features=5, max_depth = 5)\n",
    "    gb.fit(X_train_sub, y_train_sub)\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
    "    \n",
    "    #feature_importances =  gb.feature_importances_\n",
    "    #features = X_train.columns\n",
    "    #feats= pd.DataFrame({'features':features, 'importance':feature_importances})\n",
    "    #plt.figure(figsize =(8,2))\n",
    "    #plt.bar(feats.features, feats.importance, color='lightblue')\n",
    "    #plt.xticks(rotation = 90)\n",
    "    #plt.show()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "max_features = [3,4,5,6,7]\n",
    "for max_f in max_features:\n",
    "    gb = GradientBoostingClassifier(n_estimators=50, learning_rate = .1, max_features=max_f, max_depth = 5)\n",
    "    gb.fit(X_train_sub, y_train_sub)\n",
    "    print(\"max_features: \", max_f)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
    "    print()\n",
    "  \n",
    "max_depths = [3,4,5,6,7]\n",
    "for max_d in max_depths:\n",
    "    gb = GradientBoostingClassifier(n_estimators=50, learning_rate = .1, max_features=5, max_depth = max_d)\n",
    "    gb.fit(X_train_sub, y_train_sub)\n",
    "    print(\"max_depths: \", max_d)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
    "    print\n",
    "    \n",
    "n_est = [10,50,100,500,1000]\n",
    "for n in n_est:\n",
    "    gb = GradientBoostingClassifier(n_estimators=n, learning_rate = .1, max_features=5, max_depth = 5)\n",
    "    gb.fit(X_train_sub, y_train_sub)\n",
    "    print(\"n_est: \", n)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
    "    print()\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
